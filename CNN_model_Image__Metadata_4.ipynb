{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e231f5",
   "metadata": {},
   "source": [
    "# To start we will import a few of the packages we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d72d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f5f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob # this will help us download the data in order to visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f2c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions will be vital in making the CNN\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aee581",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For some image processing\n",
    "import keras.preprocessing.image as kpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77329c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will be important for processing the metadata\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbc320f",
   "metadata": {},
   "source": [
    "# With our packages imported we can now need to get the data into the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eb5d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting with down loading the meta data\n",
    "data_location = '/fs/ess/PAS2038/PHYSICS5680_OSU/student_data/armitage'\n",
    "Meta = pd.DataFrame()\n",
    "Meta_name = data_location + '/train.csv'\n",
    "Meta = pd.read_csv(Meta_name,header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c25283",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then the image data\n",
    "image_location = data_location + '/train/*.jpg'\n",
    "images = glob(image_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f45eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Meta['Random'] = np.random.randint(0,1000,size = len(Meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a451b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To test we want to consolidate these two into a single data frame\n",
    "Meta['Path'] = images\n",
    "print(Meta['Id'][0])\n",
    "Meta['Path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1d1649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "Meta_shuffled = shuffle(Meta, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ea58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Meta.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcf3513",
   "metadata": {},
   "source": [
    "# With the data imported, lets visualize it a bit to make sure it is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b92450",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    show_img = plt.imread(images[i])\n",
    "    plt.imshow(show_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae03a62a",
   "metadata": {},
   "source": [
    "# Excellent, now to resize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbccd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we can see each of these above images are different sizes so we need to resize them\n",
    "from PIL import Image\n",
    "\n",
    "reshaped_images = []\n",
    "\n",
    "for i in range(len(Meta_shuffled)):\n",
    "    pillow_image = Image.open(Meta_shuffled['Path'][i])\n",
    "    reshaped_images.append(pillow_image.resize((64,64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537ff06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing them\n",
    "image_array = []\n",
    "for i in range(len(reshaped_images)):\n",
    "    image_array.append(np.array(reshaped_images[i])/256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e4badd",
   "metadata": {},
   "source": [
    "# Perfect, looks like it was installed, so lets split these data sets into a test and train. We also need to resize these images to be able to work well with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79ef549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Splitting into X(Features) Y(Pawpularity)\n",
    "X_Meta = Meta_shuffled.iloc[:,1:13]\n",
    "Y_Meta = Meta_shuffled['Pawpularity']\n",
    "\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_Meta,Y_Meta.values, test_size=0.2, shuffle = False)\n",
    "image_train,image_test,iy_train,iy_test = train_test_split(image_array,Y_Meta.values,test_size = 0.2, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5453e302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#double checking that it worked\n",
    "for i in range(6):\n",
    "    plt.subplot(1,6,i+1)\n",
    "    plt.imshow(image_train[i])\n",
    "    plt.title(iy_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a626afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_test = np.array(image_test)\n",
    "image_train = np.array(image_train)\n",
    "print(type(image_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7af7627",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "y_train = []\n",
    "for i in range(len(Y_test)):\n",
    "    y_test.append([Y_test[i]])\n",
    "for i in range(len(Y_train)):\n",
    "    y_train.append([Y_train[i]])\n",
    "y_test = np.array(y_test)\n",
    "y_train = np.array(y_train)\n",
    "print(y_test[0])\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8af7f5a",
   "metadata": {},
   "source": [
    "# Now we can start building our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127da00a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Starting with the CNN\n",
    "CNN = models.Sequential()\n",
    "#\n",
    "# First convolutional layer\n",
    "CNN.add(layers.Conv2D(30,(5,5),activation='relu',input_shape=(64,64,3)))\n",
    "\n",
    "#Batch normalize, word on the street is, its pretty cool and helps keep from overfitting\n",
    "CNN.add(layers.BatchNormalization())\n",
    "\n",
    "#To prevent overfitting we will also use dropout cutting 20% of neurons\n",
    "CNN.add(layers.Dropout(0.2))\n",
    "\n",
    "# Pool\n",
    "CNN.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "#Normalize again\n",
    "CNN.add(layers.BatchNormalization())\n",
    "\n",
    "# Second convolutional layer\n",
    "CNN.add(layers.Conv2D(25,(5,5),activation='relu'))\n",
    "\n",
    "#Shave off a few more braincells(neurons) here\n",
    "CNN.add(layers.Dropout(0.2))\n",
    "\n",
    "#Hi I'm Normal\n",
    "CNN.add(layers.BatchNormalization())\n",
    "\n",
    "# Pool\n",
    "CNN.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "#Normal is immune to ghost, but has a weakness to fighting\n",
    "CNN.add(layers.BatchNormalization())\n",
    "\n",
    "#Layer number 3\n",
    "CNN.add(layers.Conv2D(32,(3,3),activation='relu'))\n",
    "\n",
    "# Connect to a dense output layer - just like an FCN\n",
    "CNN.add(layers.Flatten())\n",
    "CNN.add(layers.Dense(64,activation='relu'))\n",
    "CNN.add(layers.Dense(32,activation= 'relu'))\n",
    "CNN.add(layers.Dense(1,activation = 'relu'))\n",
    "\n",
    "#compiling the model\n",
    "#we choose means square error as that is the metric for the contest\n",
    "CNN.compile(optimizer='adam',loss='mean_squared_error',metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss',patience = 10)]\n",
    "print(CNN.summary())\n",
    "CNN_results = CNN.fit(image_train,iy_train,\n",
    "                          epochs=100,\n",
    "                          batch_size=256,\n",
    "                          callbacks=callbacks, # Early stopping\n",
    "                          validation_data=(image_test,iy_test)\n",
    "                     )\n",
    "\n",
    "\n",
    "CNN.save('CNN_Pets')\n",
    "\n",
    "print(CNN.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec47d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default='notebook'\n",
    "\n",
    "df_cnn = pd.DataFrame(CNN_results.history)\n",
    "df_cnn['iteration'] = df_cnn.index + 1\n",
    "#\n",
    "#\n",
    "# RMSE\n",
    "fig = px.line(df_cnn, x='iteration', y=['root_mean_squared_error','val_root_mean_squared_error'], title='RMSE vs Iteration')\n",
    "\n",
    "#\n",
    "newnames = {'root_mean_squared_error':'RMSE', 'val_root_mean_squared_error': 'Val_RMSE'}\n",
    "fig.for_each_trace(lambda t: t.update(name = newnames[t.name],\n",
    "                                      legendgroup = newnames[t.name],\n",
    "                                      hovertemplate = t.hovertemplate.replace(t.name, newnames[t.name])\n",
    "                                     )\n",
    "                  )\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Loss\n",
    "fig = px.line(df_cnn, x='iteration', y=['loss','val_loss'], title='Loss vs Iteration')\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e5e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_results.history "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50ff44c",
   "metadata": {},
   "source": [
    "\n",
    "# This next cell will check the model against the given test data or our split test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc08bbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Meta_test = pd.DataFrame()\n",
    "Meta_name_test = data_location + '/test.csv'\n",
    "Meta_test = pd.read_csv(Meta_name_test,header=0)\n",
    "\n",
    "test_image_location = data_location + '/test/*.jpg'\n",
    "test_images = glob(test_image_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde491ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    show_test_img = plt.imread(test_images[i])\n",
    "    plt.imshow(show_test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5804ce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_test_images = []\n",
    "\n",
    "for i in range(len(test_images)):\n",
    "    pillow_test_image = Image.open(test_images[i])\n",
    "    reshaped_test_images.append(pillow_test_image.resize((64,64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b13ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_array = []\n",
    "for i in range(len(reshaped_test_images)):\n",
    "    test_image_array.append(np.array(reshaped_test_images[i])/256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e4a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_array = np.array(test_image_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966bf857",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_predict = CNN.predict(image_test)\n",
    "CNN_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a9b306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I spelled this wrong but its ok because it doesnt overwrite anything\n",
    "CNN_perdict = CNN.predict(image_test)\n",
    "CNN_perdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cba5468",
   "metadata": {},
   "outputs": [],
   "source": [
    "perdict = pd.DataFrame(CNN_perdict)\n",
    "perdict['Pawpularity']= perdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e645df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "perdict['Real Pawpularity'] = iy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d66e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(perdict, barmode=\"overlay\", x=['Pawpularity','Real Pawpularity'], title='Pawpularity distribution')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a95ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
